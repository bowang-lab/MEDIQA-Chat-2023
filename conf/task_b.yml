# ModelArguments
model_name_or_path: "allenai/led-base-16384-ms2"

# DataTrainingArguments
text_column: "dialogue"
summary_column: "note"
train_file: "/home/johnmg/projects/def-wanglab-ab/johnmg/mediqa-chat-tasks-acl-2023/datasets/MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskB/TaskB-TrainingSet.csv"
validation_file: "/home/johnmg/projects/def-wanglab-ab/johnmg/mediqa-chat-tasks-acl-2023/datasets/MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskB/TaskB-ValidationSet.csv"
test_file: "/home/johnmg/projects/def-wanglab-ab/johnmg/mediqa-chat-tasks-acl-2023/datasets/MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskB/TaskB-ValidationSet.csv"
max_source_length: 3072
max_target_length: 864
# This is the maximum length of the target sequence for evaluation. It should be set to the maximum observed length
# in the validation set (or as large as the model supports, whichever is smaller) so we get an accurate evaluation.
val_max_target_length: 1024
num_beams: 3
source_prefix: "Summarize the following patient-doctor dialogue. Include all medically relevant information, including family history, diagnosis, past medical (and surgical) history, immunizations, lab results and known allergies. Dialogue:"
# Specify which challenge task to run, which changes some of the pre and post processing
task: "B"

# Seq2SeqTrainingArguments
do_train: true
do_eval: true
do_predict: true
per_device_train_batch_size: 4
per_device_eval_batch_size: 16
gradient_accumulation_steps: 2
learning_rate: 3e-5
weight_decay: 0.01
num_train_epochs: 50
warmup_ratio: 0.1
label_smoothing_factor: 0.1
fp16: true
# Controls the evaluation strategy
evaluation_strategy: "steps"
eval_steps: 25
eval_delay: 300
# Controls the checkpointing strategy
save_strategy: "steps"
save_steps: 25
save_total_limit: 1
load_best_model_at_end: true
metric_for_best_model: "rougeL"