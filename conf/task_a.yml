# ModelArguments
model_name_or_path: "google/flan-t5-large"

# DataTrainingArguments
text_column: "dialogue"
summary_column: "section_text"
train_file: "./datasets/MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskA/TaskA-TrainingSet.csv"
validation_file: "./datasets/MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskA/TaskA-ValidationSet.csv"
test_file: "./datasets/MEDIQA-Chat-Training-ValidationSets-Feb-10-2023/TaskA/TaskA-ValidationSet.csv"
max_source_length: 1024
max_target_length: 256
# This is the maximum length of the target sequence for evaluation. It should be set to the maximum observed length
# in the validation set (or as large as the model supports, whichever is smaller) so we get an accurate evaluation.
val_max_target_length: 512
num_beams: 1
source_prefix: "Summarize the following patient-doctor dialogue. Include all medically relevant information, including family history, diagnosis, past medical (and surgical) history, immunizations, lab results and known allergies. You should first predict the most relevant clinical note section header and then summarize the dialogue. Dialogue:"
task: "A"

# Seq2SeqTrainingArguments
do_train: true
do_eval: true
do_predict: true
per_device_train_batch_size: 2
per_device_eval_batch_size: 16
gradient_accumulation_steps: 4
learning_rate: 1e-4
weight_decay: 0.01
num_train_epochs: 10
warmup_ratio: 0.1
label_smoothing_factor: 0.1
bf16: true
# Controls the evaluation strategy
evaluation_strategy: "steps"
eval_steps: 250
eval_delay: 750
# Controls the checkpointing strategy
save_strategy: "steps"
save_steps: 250
save_total_limit: 1
load_best_model_at_end: true
metric_for_best_model: "rougeL"